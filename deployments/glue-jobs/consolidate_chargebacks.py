"""
AWS Glue ETL Job: Chargeback Data Consolidation
================================================

Purpose:
    Consolidates thousands of small Parquet files from the landing zone
    (generated by Flink 1:1 processing) into optimized, partitioned datasets
    for efficient querying and analytics.

Input:
    - Source: S3 landing zone with small Parquet files (50-100 KB each)
    - Path: s3://bucket/landing/chargebacks/YYYY/MM/DD/*.parquet
    - Volume: ~1.25M records per execution (for 5M/day with 4 executions)

Output:
    - Destination: S3 consolidated zone
    - Path: s3://bucket/consolidated/chargebacks/year=YYYY/month=MM/day=DD/*
    - Format: CSV, Parquet, or JSON (parametrized)
    - Files: N consolidated files (parametrized, default 10)
    - Size: ~50-100 MB per file (optimal for analytics)

Processing:
    1. Read all landing zone files for the current partition
    2. Union into single DataFrame
    3. Perform data quality checks
    4. Add metadata and deduplicate
    5. Repartition to desired file count
    6. Write as CSV/Parquet/JSON
    7. Send consolidation event to Kafka (optional)
    8. Log metrics and completion

Kafka Integration:
    - Sends consolidation completion events to MSK topic
    - Event includes: partition date, records processed, output path
    - Can be consumed by Lambda to update DynamoDB chargeback status

Author: AWS Glue ETL
Version: 2.0.0
Glue Version: 3.0 (Spark 3.1, Python 3.7)
"""

import sys
from datetime import datetime, timedelta
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from pyspark.sql import functions as F
from pyspark.sql import Window
from pyspark.sql.types import *

# =============================================================================
# CONFIGURATION AND ARGUMENTS
# =============================================================================

# Required arguments
args = getResolvedOptions(sys.argv, [
    'JOB_NAME',
    'SOURCE_DATABASE',
    'SOURCE_TABLE',
    'OUTPUT_PATH',
    'OUTPUT_FILE_COUNT',
    'OUTPUT_FORMAT'
])

# Optional arguments with defaults
optional_args = {
    'EXECUTION_TIME': datetime.now(datetime.timezone.utc).strftime('%Y-%m-%dT%H:%M:%S'),
    'EXECUTION_SEQUENCE': '1',
    'TOTAL_EXECUTIONS': '4',
    'ENABLE_PARTITION_FILTER': 'true',
    'PARTITION_DATE': None,  # Auto-calculate if not provided
    'DRY_RUN': 'false',
    'COMPRESSION_CODEC': 'snappy',  # Only used for Parquet
    'CSV_DELIMITER': ',',
    'CSV_HEADER': 'true',
    'CSV_QUOTE_CHAR': '"',
    'ENABLE_KAFKA': 'false',
    'KAFKA_BOOTSTRAP_SERVERS': '',
    'KAFKA_TOPIC': 'chargeback-consolidation-events'
}

for key, default in optional_args.items():
    try:
        args[key] = getResolvedOptions(sys.argv, [key])[key]
    except:
        args[key] = default

# Parse arguments
SOURCE_DATABASE = args['SOURCE_DATABASE']
SOURCE_TABLE = args['SOURCE_TABLE']
OUTPUT_PATH = args['OUTPUT_PATH']
OUTPUT_FILE_COUNT = int(args['OUTPUT_FILE_COUNT'])
OUTPUT_FORMAT = args['OUTPUT_FORMAT'].lower()  # csv, parquet, json
COMPRESSION_CODEC = args['COMPRESSION_CODEC']
CSV_DELIMITER = args['CSV_DELIMITER']
CSV_HEADER = args['CSV_HEADER'].lower() == 'true'
CSV_QUOTE_CHAR = args['CSV_QUOTE_CHAR']
EXECUTION_TIME = args['EXECUTION_TIME']
EXECUTION_SEQUENCE = int(args['EXECUTION_SEQUENCE'])
TOTAL_EXECUTIONS = int(args['TOTAL_EXECUTIONS'])
ENABLE_PARTITION_FILTER = args['ENABLE_PARTITION_FILTER'].lower() == 'true'
DRY_RUN = args['DRY_RUN'].lower() == 'true'
ENABLE_KAFKA = args['ENABLE_KAFKA'].lower() == 'true'
KAFKA_BOOTSTRAP_SERVERS = args['KAFKA_BOOTSTRAP_SERVERS']
KAFKA_TOPIC = args['KAFKA_TOPIC']

# Calculate partition date (yesterday's data, or specified)
if args['PARTITION_DATE']:
    partition_date = datetime.strptime(args['PARTITION_DATE'], '%Y-%m-%d')
else:
    # Default: process yesterday's data
    partition_date = datetime.utcnow() - timedelta(days=1)

PARTITION_YEAR = partition_date.strftime('%Y')
PARTITION_MONTH = partition_date.strftime('%m')
PARTITION_DAY = partition_date.strftime('%d')

# =============================================================================
# INITIALIZE SPARK AND GLUE CONTEXTS
# =============================================================================

sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# Configure Spark for optimal performance
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
spark.conf.set("spark.sql.files.maxPartitionBytes", "134217728")  # 128 MB
spark.conf.set("spark.sql.shuffle.partitions", str(OUTPUT_FILE_COUNT * 2))

# Log configuration
print("=" * 80)
print("GLUE JOB CONFIGURATION")
print("=" * 80)
print(f"Job Name: {args['JOB_NAME']}")
print(f"Execution Time: {EXECUTION_TIME}")
print(f"Execution Sequence: {EXECUTION_SEQUENCE} of {TOTAL_EXECUTIONS}")
print(f"Source Database: {SOURCE_DATABASE}")
print(f"Source Table: {SOURCE_TABLE}")
print(f"Output Path: {OUTPUT_PATH}")
print(f"Output File Count: {OUTPUT_FILE_COUNT}")
print(f"Compression Codec: {COMPRESSION_CODEC}")
print(f"Partition: year={PARTITION_YEAR}/month={PARTITION_MONTH}/day={PARTITION_DAY}")
print(f"Partition Filter Enabled: {ENABLE_PARTITION_FILTER}")
print(f"Dry Run: {DRY_RUN}")
print("=" * 80)

# =============================================================================
# READ DATA FROM GLUE CATALOG
# =============================================================================

print(f"\n[1/7] Reading data from Glue Catalog...")
print(f"Output Format: {OUTPUT_FORMAT.upper()}")

try:
    # Build push-down predicate for partition filtering
    if ENABLE_PARTITION_FILTER:
        push_down_predicate = f"(year='{PARTITION_YEAR}' and month='{PARTITION_MONTH}' and day='{PARTITION_DAY}')"
        print(f"Applying partition filter: {push_down_predicate}")
        
        # Read from Glue Catalog with partition pruning
        datasource = glueContext.create_dynamic_frame.from_catalog(
            database=SOURCE_DATABASE,
            table_name=SOURCE_TABLE,
            push_down_predicate=push_down_predicate,
            transformation_ctx="datasource"
        )
    else:
        # Read all data (not recommended for large datasets)
        print("WARNING: Reading all partitions (no filter applied)")
        datasource = glueContext.create_dynamic_frame.from_catalog(
            database=SOURCE_DATABASE,
            table_name=SOURCE_TABLE,
            transformation_ctx="datasource"
        )
    
    # Convert to Spark DataFrame for better control
    df = datasource.toDF()
    
    # Check if data exists
    record_count = df.count()
    
    if record_count == 0:
        print(f"WARNING: No data found for partition year={PARTITION_YEAR}/month={PARTITION_MONTH}/day={PARTITION_DAY}")
        print("Job will complete successfully but no output will be written.")
        job.commit()
        sys.exit(0)
    
    print(f"✓ Successfully read {record_count:,} records from landing zone")
    print(f"  Schema: {len(df.columns)} columns")
    print(f"  Partitions: {df.rdd.getNumPartitions()}")
    
except Exception as e:
    print(f"ERROR: Failed to read from Glue Catalog: {str(e)}")
    raise

# =============================================================================
# DATA QUALITY CHECKS
# =============================================================================

print("\n[2/7] Performing data quality checks...")

# Check for null chargeback_ids
null_ids = df.filter(F.col("chargeback_id").isNull()).count()
if null_ids > 0:
    print(f"WARNING: Found {null_ids} records with null chargeback_id")

# Check for duplicates
duplicate_count = df.groupBy("chargeback_id").count().filter(F.col("count") > 1).count()
if duplicate_count > 0:
    print(f"WARNING: Found {duplicate_count} duplicate chargeback_ids")

# Log event type distribution
print("\nEvent Type Distribution:")
event_distribution = df.groupBy("event_type").count().collect()
for row in event_distribution:
    print(f"  {row['event_type']}: {row['count']:,} records")

# Log status distribution
print("\nStatus Distribution:")
status_distribution = df.groupBy("status").count().collect()
for row in status_distribution:
    print(f"  {row['status']}: {row['count']:,} records")

print("✓ Data quality checks completed")

# =============================================================================
# DATA TRANSFORMATIONS (if needed)
# =============================================================================

print("\n[3/7] Adding metadata and deduplication...")

# Add processing metadata
df_transformed = df.withColumn(
    "consolidated_at",
    F.lit(EXECUTION_TIME).cast(TimestampType())
).withColumn(
    "consolidation_job",
    F.lit(args['JOB_NAME'])
).withColumn(
    "execution_sequence",
    F.lit(EXECUTION_SEQUENCE)
)

# Deduplicate by chargeback_id (keep latest by updated_at)
print("Deduplicating records...")
df_deduped = df_transformed.withColumn(
    "row_num",
    F.row_number().over(
        Window.partitionBy("chargeback_id").orderBy(F.col("updated_at").desc())
    )
).filter(F.col("row_num") == 1).drop("row_num")

deduped_count = df_deduped.count()
removed_duplicates = record_count - deduped_count

print(f"✓ Removed {removed_duplicates:,} duplicate records")
print(f"✓ Final record count: {deduped_count:,}")

# =============================================================================
# REPARTITION FOR OPTIMAL FILE SIZE
# =============================================================================

print(f"\n[4/7] Repartitioning data to {OUTPUT_FILE_COUNT} files...")

# Repartition by hash of chargeback_id for even distribution
df_repartitioned = df_deduped.repartition(OUTPUT_FILE_COUNT, "chargeback_id")

print(f"✓ Repartitioned to {df_repartitioned.rdd.getNumPartitions()} partitions")

# =============================================================================
# WRITE CONSOLIDATED PARQUET FILES
# =============================================================================

print(f"\n[5/7] Writing consolidated {OUTPUT_FORMAT.upper()} files...")

if DRY_RUN:
    print("DRY RUN MODE: Skipping write operation")
    print(f"Would write to: {OUTPUT_PATH}")
else:
    try:
        # Output path with date partitioning
        output_path_with_partition = f"{OUTPUT_PATH}/year={PARTITION_YEAR}/month={PARTITION_MONTH}/day={PARTITION_DAY}"
        
        print(f"Output path: {output_path_with_partition}")
        print(f"Format: {OUTPUT_FORMAT}")
        print(f"Expected files: {OUTPUT_FILE_COUNT}")
        
        # Write based on format
        if OUTPUT_FORMAT == "csv":
            print(f"CSV Delimiter: '{CSV_DELIMITER}'")
            print(f"CSV Header: {CSV_HEADER}")
            
            df_repartitioned.write \
                .mode("overwrite") \
                .format("csv") \
                .option("header", str(CSV_HEADER).lower()) \
                .option("delimiter", CSV_DELIMITER) \
                .option("quote", CSV_QUOTE_CHAR) \
                .option("escape", "\\") \
                .option("quoteMode", "MINIMAL") \
                .save(output_path_with_partition)
            
            print("✓ Successfully wrote consolidated CSV files")
            
            # Verify output
            output_files = spark.read.csv(output_path_with_partition, header=CSV_HEADER)
            output_count = output_files.count()
            
        elif OUTPUT_FORMAT == "parquet":
            print(f"Compression: {COMPRESSION_CODEC}")
            
            df_repartitioned.write \
                .mode("overwrite") \
                .format("parquet") \
                .option("compression", COMPRESSION_CODEC) \
                .option("parquet.block.size", 134217728) \
                .option("parquet.page.size", 1048576) \
                .save(output_path_with_partition)
            
            print("✓ Successfully wrote consolidated Parquet files")
            
            # Verify output
            output_files = spark.read.parquet(output_path_with_partition)
            output_count = output_files.count()
            
        elif OUTPUT_FORMAT == "json":
            df_repartitioned.write \
                .mode("overwrite") \
                .format("json") \
                .save(output_path_with_partition)
            
            print("✓ Successfully wrote consolidated JSON files")
            
            # Verify output
            output_files = spark.read.json(output_path_with_partition)
            output_count = output_files.count()
            
        else:
            raise ValueError(f"Unsupported output format: {OUTPUT_FORMAT}")
        
        print(f"✓ Verification: Output contains {output_count:,} records")
        
        if output_count != deduped_count:
            print(f"WARNING: Output record count ({output_count}) does not match input ({deduped_count})")
        
    except Exception as e:
        print(f"ERROR: Failed to write output: {str(e)}")
        raise

# =============================================================================
# SEND KAFKA NOTIFICATION (if enabled)
# =============================================================================

print("\n[6/7] Sending consolidation event to Kafka...")

if ENABLE_KAFKA and KAFKA_BOOTSTRAP_SERVERS:
    try:
        import json
        import boto3
        from datetime import timezone
        
        # Prepare consolidation event message
        consolidation_event = {
            "event_type": "consolidation_completed",
            "partition_date": f"{PARTITION_YEAR}-{PARTITION_MONTH}-{PARTITION_DAY}",
            "execution_sequence": EXECUTION_SEQUENCE,
            "total_executions": TOTAL_EXECUTIONS,
            "records_processed": deduped_count,
            "duplicates_removed": removed_duplicates,
            "output_files": OUTPUT_FILE_COUNT,
            "output_format": OUTPUT_FORMAT,
            "output_path": output_path_with_partition,
            "execution_time": EXECUTION_TIME,
            "completed_at": datetime.now(timezone.utc).isoformat(),
            "job_name": args['JOB_NAME']
        }
        
        # Convert to JSON
        message_value = json.dumps(consolidation_event)
        message_key = f"{PARTITION_YEAR}-{PARTITION_MONTH}-{PARTITION_DAY}"
        
        print(f"Kafka Topic: {KAFKA_TOPIC}")
        print(f"Kafka Bootstrap Servers: {KAFKA_BOOTSTRAP_SERVERS}")
        print(f"Message Key: {message_key}")
        
        # Use kafka-python library (available in Glue 3.0)
        from kafka import KafkaProducer
        from kafka.errors import KafkaError
        
        # Create producer with IAM authentication
        producer = KafkaProducer(
            bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS.split(','),
            value_serializer=lambda v: v.encode('utf-8'),
            key_serializer=lambda k: k.encode('utf-8') if k else None,
            security_protocol='SASL_SSL',
            sasl_mechanism='AWS_MSK_IAM',
            sasl_oauth_token_provider=lambda: boto3.client('sts').get_caller_identity(),
            acks='all',
            retries=3,
            max_in_flight_requests_per_connection=1
        )
        
        # Send message
        future = producer.send(
            KAFKA_TOPIC,
            key=message_key,
            value=message_value
        )
        
        # Wait for send to complete
        record_metadata = future.get(timeout=10)
        
        print(f"✓ Successfully sent consolidation event to Kafka")
        print(f"  Topic: {record_metadata.topic}")
        print(f"  Partition: {record_metadata.partition}")
        print(f"  Offset: {record_metadata.offset}")
        
        # Close producer
        producer.close()
        
    except ImportError as e:
        print(f"WARNING: kafka-python library not available: {str(e)}")
        print("Consolidation event not sent to Kafka")
    except Exception as e:
        print(f"ERROR: Failed to send Kafka message: {str(e)}")
        print("Continuing despite Kafka error (non-critical)")
else:
    if not ENABLE_KAFKA:
        print("Kafka notifications disabled")
    else:
        print("WARNING: Kafka enabled but bootstrap servers not configured")

# =============================================================================
# LOG METRICS AND SUMMARY
# =============================================================================

print("\n[7/7] Logging metrics and summary...")

# Calculate metrics
execution_end_time = datetime.utcnow()
execution_start_time = datetime.fromisoformat(EXECUTION_TIME.replace('Z', ''))
execution_duration_seconds = (execution_end_time - execution_start_time).total_seconds()

# Estimate file sizes (approximate)
avg_record_size_bytes = 100  # Estimated compressed size per record
total_size_mb = (deduped_count * avg_record_size_bytes) / 1024 / 1024
avg_file_size_mb = total_size_mb / OUTPUT_FILE_COUNT

print("\n" + "=" * 80)
print("CONSOLIDATION SUMMARY")
print("=" * 80)
print(f"Partition: {PARTITION_YEAR}-{PARTITION_MONTH}-{PARTITION_DAY}")
print(f"Input Records: {record_count:,}")
print(f"Duplicates Removed: {removed_duplicates:,}")
print(f"Output Records: {deduped_count:,}")
print(f"Output Format: {OUTPUT_FORMAT.upper()}")
print(f"Output Files: {OUTPUT_FILE_COUNT}")
print(f"Estimated Total Size: {total_size_mb:.2f} MB")
print(f"Estimated Avg File Size: {avg_file_size_mb:.2f} MB")
if OUTPUT_FORMAT == "parquet":
    print(f"Compression: {COMPRESSION_CODEC}")
elif OUTPUT_FORMAT == "csv":
    print(f"CSV Delimiter: '{CSV_DELIMITER}'")
    print(f"CSV Header: {CSV_HEADER}")
print(f"Execution Duration: {execution_duration_seconds:.2f} seconds")
print(f"Processing Rate: {deduped_count / execution_duration_seconds if execution_duration_seconds > 0 else 0:.0f} records/sec")
if ENABLE_KAFKA and KAFKA_BOOTSTRAP_SERVERS:
    print(f"Kafka Notification: Sent to {KAFKA_TOPIC}")
print("=" * 80)

# Log for CloudWatch Logs Insights parsing
print(f"METRICS: records_processed={deduped_count}, duplicates_removed={removed_duplicates}, output_files={OUTPUT_FILE_COUNT}, output_format={OUTPUT_FORMAT}, duration_seconds={execution_duration_seconds:.2f}, kafka_sent={ENABLE_KAFKA and KAFKA_BOOTSTRAP_SERVERS != ''}")

print("\n✓ Consolidation completed successfully!")

# =============================================================================
# COMMIT JOB
# =============================================================================

job.commit()

print("\n" + "=" * 80)
print(f"Job {args['JOB_NAME']} completed successfully")
print(f"Execution Sequence: {EXECUTION_SEQUENCE} of {TOTAL_EXECUTIONS}")
print(f"Next execution: {EXECUTION_SEQUENCE + 1 if EXECUTION_SEQUENCE < TOTAL_EXECUTIONS else 'N/A (last execution of the day)'}")
print("=" * 80)
